{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e2ae43-d2bd-4fad-b1f5-ef751ed3e56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Solo implementation of a spatial transformer network including a process by which the \n",
    "# an image can be inserted and readjusted before transformation\n",
    "# Although this is a functioning code example of a spatial transformer, it is currently \n",
    "# unable to generate educated transformations on the images, because it is not yet \n",
    "# integrated within a CNN. For the STN to properly learn how to perform the best transformation\n",
    "# to diminish spatial invariance, it must learn from the loss function value which comes from the \n",
    "# implementation of the CNN. Without gradient descenet and backpropogation, the STN has no \n",
    "# delimiter to make its decisions on.\n",
    "# Regardless, we will explore the structure and foundational aspects of the STN to better understand\n",
    "# how these transformations are executed.\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6995f6c5-312c-43a0-812b-f513dd30d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input shape as a set size\n",
    "input_shape = [224,224,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dae5bf5-7caf-40b7-bdb9-02da74abbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ulterior function that is not a fundamental to all STNs, responsible for \n",
    "# resizing the image\n",
    "# Aspect ratio of an image is the height to wdith ratio\n",
    "# Maintaining the aspect ratio helps avoid distortion of the image\n",
    "def resize_image_keep_aspect_ratio(image, target_height, target_width):\n",
    "    original_height = tf.shape(image)[0]\n",
    "    original_width = tf.shape(image)[1]\n",
    "    # establish the aspect ratio\n",
    "    ratio = tf.cast(original_width / original_height, tf.float32)\n",
    "    # set the height to the target height\n",
    "    new_height = target_height\n",
    "    # set the width to the target height multiplied by the aspect ratio\n",
    "    new_width = tf.cast(target_height * ratio, tf.int32)\n",
    "\n",
    "    # crop to ensure final image has same constraints, cropping keeps the\n",
    "    # center of the image thus preserving the main content\n",
    "    image = tf.image.resize(image, (new_height, new_width))\n",
    "    image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585b8e56-fa1c-4f8e-bcf5-dce4b162aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that applies the spatial transformation to the image based on the \n",
    "# transformation parameters defined in theta.\n",
    "# input_fmap is the feature map to which the spatial transformation will be applied\n",
    "# theta is the affine transformation matrix learned in the localization net\n",
    "    # affine transformation matrix is a geometric transformation that preserves lines and \n",
    "    # parallelism (preserving collinearity)\n",
    "# out_dims is the dimensions of the feature map and is set to the original dimensions if \n",
    "    # no value is provided\n",
    "def spatial_transformer_network(input_fmap, theta, out_dims=None, **kwargs):\n",
    "    # access the batch size (B), the height (H), and the width (W)\n",
    "    B = tf.shape(input_fmap)[0]\n",
    "    H = tf.shape(input_fmap)[1]\n",
    "    W = tf.shape(input_fmap)[2]\n",
    "\n",
    "    # reshaping theta to (B, 2, 3) so it matches expected format\n",
    "    theta = tf.reshape(theta, [B, 2, 3])\n",
    "\n",
    "    # generate a grid based on the dimensions\n",
    "    if out_dims:\n",
    "        out_H = out_dims[0]\n",
    "        out_W = out_dims[1]\n",
    "        batch_grids = affine_grid_generator(out_H, out_W, theta)\n",
    "    else:\n",
    "        batch_grids = affine_grid_generator(H, W, theta)\n",
    "\n",
    "    x_s = batch_grids[:,0,:,:]\n",
    "    y_s = batch_grids[:,1,:,:]\n",
    "    \n",
    "    # bilinear sampler samples the input at the given x and y coordinates\n",
    "    # to create the output feature map\n",
    "    out_fmap = bilinear_sampler(input_fmap, x_s, y_s)\n",
    "\n",
    "    return out_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05efa2d-cceb-4306-b371-7141e4cee70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the parameters of the spatial transformation that should be applied to\n",
    "    # feature map\n",
    "# Final output will be the parameters needed to transform the input feature map \n",
    "    # in a way that limits loss function value\n",
    "def build_localization_network(input_shape):\n",
    "    # linear stack of layers, where each layer has exactly one tensor input and output\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Two convolutional layers are defined, both proceeded by max pooling\n",
    "    # Max pooling operation reports the maximum output within a rectangular neighborhood\n",
    "    # Max pooling with pool size of 2x2 reduces the spatial dimensions of the convolutional\n",
    "        # layers which helps to reduce computation and control overfitting\n",
    "    model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Fully connected layers\n",
    "    # Reshape the 2D feature map into a 1D Tensor for insertion into the dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "    # Output layer with 6 units, initialized to the identity transformation\n",
    "        # the 6 units correspond to the paramaters of the affine tranformation\n",
    "        # matrix which is 2x3\n",
    "    model.add(layers.Dense(6, activation='linear', \n",
    "                           kernel_initializer='zeros', \n",
    "                           bias_initializer=tf.constant_initializer([1, 0, 0, 0, 1, 0])))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7348a99-8c70-41a7-90a6-3bb39ebd7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that extracts pixel values from an input feature map tensor \n",
    "    # based on specific x and y coordinates\n",
    "def get_pixel_value(img, x, y):\n",
    "    shape = tf.shape(x)\n",
    "    batch_size = shape[0]\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "\n",
    "    batch_idx = tf.range(0, batch_size)\n",
    "    batch_idx = tf.reshape(batch_idx, (batch_size,1,1))\n",
    "    b = tf.tile(batch_idx, (1, height, width))\n",
    "\n",
    "    indices = tf.stack([b,y,x], 3)\n",
    "\n",
    "    return tf.gather_nd(img, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6982467a-fe20-490b-b9a6-42c70b511802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid generator that creates a grid of coordinates that are used to sample \n",
    "    # from an input feature map according to theta\n",
    "# Effectively applies the learned spatial transformation to the input data\n",
    "# Desired height, width and theta are the paramaters\n",
    "def affine_grid_generator(height, width, theta):\n",
    "    num_batch = tf.shape(theta)[0]\n",
    "\n",
    "    x = tf.linspace(-1.0,1.0, width)\n",
    "    y = tf.linspace(-1.0,1.0, height)\n",
    "    # create a grid of (x,y) coordinates that represents the output feature map\n",
    "    x_t, y_t = tf.meshgrid(x,y)\n",
    "\n",
    "    # create 1D array of all coords in output feature map and add a third row\n",
    "        # of ones so the affine transformation matrix (theta) can be properly \n",
    "        # multiplied to the grid\n",
    "    x_t_flat = tf.reshape(x_t, [-1])\n",
    "    y_t_flat = tf.reshape(y_t, [-1])\n",
    "\n",
    "    # create a third row of ones for the homogenuous coordinate system\n",
    "    ones = tf.ones_like(x_t_flat)\n",
    "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
    "\n",
    "    # expand the dimensions of the grid and replicate it for the whole batch\n",
    "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
    "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
    "\n",
    "    # certifty that both theta and the sampling grid are float 32 tensors\n",
    "    theta = tf.cast(theta, 'float32')\n",
    "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
    "\n",
    "    # apply the affine transformation to the sampling grid\n",
    "    batch_grids = tf.matmul(theta, sampling_grid)\n",
    "\n",
    "    # reshape back to a 4D tensor\n",
    "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "\n",
    "    # return batch of transformed grids\n",
    "    return batch_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f328b2b-3abb-4230-9bbf-8b9563741e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that carries out bilinear interpolation and allows for smooth transformations\n",
    "    # and resampling of the image\n",
    "def bilinear_sampler(img, x, y):\n",
    "    # instantiate the height and weight of the image\n",
    "    H = tf.shape(img)[1]\n",
    "    W = tf.shape(img)[2]\n",
    "    max_y = tf.cast(H - 1, 'int32')\n",
    "    max_x = tf.cast(W - 1, 'int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    # normalize the x and y coordinates to be within the image dimensions\n",
    "    x = tf.cast(x, 'float32')\n",
    "    y = tf.cast(y, 'float32')\n",
    "    x = 0.5 * ((x + 1.0) * tf.cast(max_x-1, 'float32'))\n",
    "    y = 0.5 * ((y + 1.0) * tf.cast(max_y-1, 'float32'))\n",
    "\n",
    "    # this is the first essential step of bilinear interpolation, the four integer,\n",
    "        # coordinates around each (x,y) coordinate will be identified for further sampling\n",
    "        # thus creating a box like shape around each coordinate.\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # clip coordinates to be within the image dimensions\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    # using the helper method get_pixel_value, obtain the values at each corner\n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "\n",
    "    # convert corner coordinates back to floats for interpolation\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    # weights for interpolation according to each corner coordinate\n",
    "    wa = (x1-x) * (y1-y)\n",
    "    wb = (x1-x) * (y-y0)\n",
    "    wc = (x-x0) * (y1-y)\n",
    "    wd = (x-x0) * (y-y0)\n",
    "\n",
    "    # expanding the dimensions\n",
    "    wa = tf.expand_dims(wa, axis=3)\n",
    "    wb = tf.expand_dims(wb, axis=3)\n",
    "    wc = tf.expand_dims(wc, axis=3)\n",
    "    wd = tf.expand_dims(wd, axis=3)\n",
    "\n",
    "    # computing the weighted sum of pixel values\n",
    "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3c23a8-dfca-43aa-a8c9-48656f33259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that allows image to be passed into STN\n",
    "    # Not essential foundational part of STN configuration\n",
    "img = Image.open(\"birdtraining9.jpeg\")\n",
    "img_array = np.array(img)\n",
    "\n",
    "img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)\n",
    "\n",
    "target_height = 224  \n",
    "target_width = 224   \n",
    "resized_img_tensor = resize_image_keep_aspect_ratio(img_tensor, target_height, target_width)\n",
    "\n",
    "resized_img_tensor = resized_img_tensor / 255.0\n",
    "\n",
    "resized_img_tensor = tf.expand_dims(resized_img_tensor, axis=0)\n",
    "\n",
    "localization_network = build_localization_network(input_shape)\n",
    "\n",
    "theta = localization_network(resized_img_tensor)\n",
    "\n",
    "transformed_image_tensor = spatial_transformer_network(resized_img_tensor, theta)\n",
    "\n",
    "transformed_image_array = transformed_image_tensor.numpy() * 255\n",
    "transformed_image_array = transformed_image_array.astype(np.uint8)\n",
    "\n",
    "if len(transformed_image_array.shape) == 4:\n",
    "    transformed_image_array = transformed_image_array[0]\n",
    "\n",
    "\n",
    "transformed_image = Image.fromarray(transformed_image_array)\n",
    "\n",
    "transformed_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
